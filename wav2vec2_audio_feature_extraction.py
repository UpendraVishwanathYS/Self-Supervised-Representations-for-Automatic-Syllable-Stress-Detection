# -*- coding: utf-8 -*-
"""wav2vec2_audio_feature_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qzhkNz40SlKD_kL8pEQCsIlr4YzcsXYn
"""

# Required librarie:
from utils import *

class Wav2Vec2_EXP_Config:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
wav2vec2_config = Wav2Vec2_EXP_Config()

try:
  parser = argparse.ArgumentParser(description="Parse model name argument")
  parser.add_argument('--model_name', type=str, default="facebook/wav2vec2-base-960h", help='Name of the model to use')
  args = parser.parse_args()
  wav2vec_model_name = args.model_name
except:
  wav2vec_model_name = "facebook/wav2vec2-base-960h"
print(f"Using model: {wav2vec_model_name}")

#wav2vec_model_name = "facebook/wav2vec2-base-960h"
wav2vec_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(wav2vec_model_name,output_hidden_states=True)
wav2vec_model = Wav2Vec2Model.from_pretrained(wav2vec_model_name,output_hidden_states=True).to(wav2vec2_config.device)

class Wav2Vec2FeatureExtraction:
    def __init__(self, feature_extractor, model, device):
        self.feature_extractor = feature_extractor
        self.model = model
        self.device = device

    def load_audio_file(self, path_to_audio_file = './', sample_rate=16000):

        """
        Load the audio file (or generate dummy audio).

        Args:
        path_to_audio_file (str/Tensor): Path to the audio file (or) waveform of the audio.
        sample_rate (int): The sample rate of the audio file.

        Returns:
        waveform (Tensor): The waveform tensor.
        sample_rate (int): The sample rate of the audio file.
        y (ndarray): The audio time series.
        duration (float): Duration of the audio.
        """
        try:
          waveform, sample_rate = torchaudio.load(path_to_audio_file)
        except:
          waveform = path_to_audio_file

        waveform = waveform.to(self.device)
        y = waveform.squeeze().cpu().numpy()
        duration = len(y) / sample_rate
        return waveform, sample_rate, y, duration

    def global_feature_extraction(self, waveform, sample_rate):

        """
        Extract features of audio signal using the pretrained model.

        Args:
        waveform (Tensor): The waveform tensor.
        sample_rate (int): The sample rate of the audio file.

        Returns:
        features_np (ndarray): The extracted features as a numpy array.
        """

        inputs = self.feature_extractor(waveform.squeeze(), sampling_rate=sample_rate, return_tensors="pt", padding=True)
        inputs = inputs['input_values']
        with torch.no_grad():
            features = self.model(inputs.to(self.device)).hidden_states

        features_np = np.array([hs.cpu().numpy() for hs in features])
        return features_np

    def frame_level_feature_extraction(self, features_np, duration, start_time, end_time, save_feature=False, save_feature_path=''):

        """
        Extract syllable-level features and save them as .npy files.

        Args:
        features_np (ndarray): The extracted audio features as a numpy array.
        duration (float): The duration of the audio file in seconds.
        start_time (float): The start time of the syllable.
        end_time (float): The end time of the syllable.
        save_feature (bool): Flag to save the features as .npy file.
        save_feature_path (str): Path to save the .npy file.

        Returns:
        syllable_features (ndarray): Extract syllable-level features.
        """

        duration_per_frame = duration / features_np.shape[-2]
        start_frame_number = int(start_time / duration_per_frame)
        end_frame_number = int(end_time / duration_per_frame)
        syllable_features = np.mean(features_np[:, :, start_frame_number:end_frame_number, :], axis=2)

        print(f'Number of frames for the syllable:{end_frame_number-start_frame_number}')

        if save_feature:
            file_name_parts = [str(start_time), str(end_time)]
            npy_file_name = f"{'#'.join(file_name_parts)}.npy"
            npy_file_path = os.path.join(save_feature_path, npy_file_name)
            np.save(npy_file_path, syllable_features)

        return syllable_features

if __name__ == "__main__":

    feature_extractor = wav2vec_feature_extractor
    model = wav2vec_model
    device = wav2vec2_config.device

    wav2vec2_feature_extractor = Wav2Vec2FeatureExtraction(feature_extractor, model, device)

    # Generate sine wave audio as an example (1 second, 16000 Hz sample rate)
    sample_rate = 16000
    duration = 1.0
    time = torch.linspace(0, duration, int(sample_rate * duration))
    frequency = 440
    dummy_audio_waveform = torch.sin(2 * np.pi * frequency * time)

    # Load the audio
    waveform, sample_rate, y, duration = wav2vec2_feature_extractor.load_audio_file(dummy_audio_waveform)

    # Extract global features
    features_np = wav2vec2_feature_extractor.global_feature_extraction(waveform, sample_rate)

    print("Shape of Extracted audio-level features:", features_np.shape)

    start_time = random.uniform(0, duration - 0.1)  # Random start time
    end_time = start_time + random.uniform(0.1, duration - start_time)  # Random end time

    # Extract frame-level features and save them
    syllable_features = wav2vec2_feature_extractor.frame_level_feature_extraction(features_np, duration, start_time, end_time, save_feature=False, save_feature_path='./')

    # Print the result
    print("Shape of Extracted syllable-level features:", syllable_features.shape)